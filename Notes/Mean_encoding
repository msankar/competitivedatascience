Label encoding gives random order. No correlation with
target
2. Mean encoding helps to separate zeros from ones

Ways to use target variable
Goods - number of ones in a group,
Bads - number of zeros
 likelihood = goods / (good + bad )
 weight of evidence = ln (goods/bad)
 count = goods
 diff = goods - bads
==================================
 Regularization

 1. CrossVal (CV) loop inside training data;
 • Robust and intuitive
• Usually decent results with 4-5 folds across different datasets
• Need to be careful with extreme situations like LOO
• Perfect feature for LOO scheme
• Target variable leakage is still present even for KFold scheme

2. Smoothing;
Alpha controls the amount of regularization
• Only works together with some other regularization method
Smoothing
(mean(target) * nrows+globalmean*alpha) / (nrows+alpha)

3. Adding random noise;
Noise degrades the quality of encoding
• How much noise should we add?
• Usually used together with LOO

4. Sorting and calculating expanding mean.
Least amount of leakage
• No hyper parameters
• Irregular encoding quality
• Built - in in CatBoost
There are a lot ways to regularize mean encodings
• Unending battle with target variable leakage
• CV loop or Expanding mean for practical tasks
=============================================
Generalizations and extensions
• Using target variable in different tasks. Regression, multiclass
• Domains with many-to-many relations
• Timeseries
• Encoding interactions and numerical feature

Regression and multiclass
• More statistics for regression tasks. Percentiles, std, distribution bins.
• Introducing new information for one vs all classifiers in multi class tasks

Many-to-many relations
• Cross product of entities
• Statistics from vectors

Time series
• Time structure allows us to make a lot of complicated features.
• Rolling statistics of target variable

Correct validation reminder
• Local experiments:
‒ Estimate encodings on X_tr
‒ Map them to X_tr and X_val
‒ Regularize on X_tr
‒ Validate model on X_tr/ X_val split
• Submission:
‒ Estimate encodings on whole Train data
‒ Map them to Train and Test
‒ Regularize on Train
‒ Fit on Train


Main advantages:
‒ Compact transformation of categorical variables
‒ Powerful basis for feature engineering
• Disadvantages:
‒ Need careful validation, there a lot of ways to overfit
‒ Significant improvements only on specific datasets
==========================================

Question 3
What is the correct way of validation when doing mean encodings?
Correct answers:

First split the data into train and validation, then estimate encodings on train, then apply them to validation, then validate the model on that split. That way we avoid target variable leakage.
Incorrect answers:

Fix cross-validation split, use that split to calculate mean encodings with CV-loop regularization, use the same split to validate the model. This way we will overfit, because target from validation is used to calculate mean encodings on train. So the model implicitly uses this information which results in mild target leakage.
Calculate mean encodings on all train data, regularize them, then validate your model on random validation split. This way we will overfit even more, because target from validation is explicitly used to calculate mean encodings.
Question 4
Suppose we have a data frame 'df' with categorical variable 'item_id' and target variable 'target'.We create 2 different mean encodings:
1)via df['item_id_encoded1'] = df.groupby('item_id')['target'].transform('mean')
2)via OneHotEncoding item_id, fitting Linear Regression on one hot-encoded version of item_id and then calculating 'item_id_encoded2' as a prediction from this linear regression on the same data.
Correct answers:

'item_id_encoded1' and 'item_id_encoded2' will be essentially the same only if linear regression was fitted without a regularization. Remember, after one hot encoding there will be only one '1' in each row and the rest - zeros. Since we don't have a regularization, coefficients of each variable would be target means of item_id corresponding to that variable.
Incorrect answers:

'item_id_encoded1' and 'item_id_encoded2' will be essentially the same. Thats not always true. With regularization it will not converge to the same values as 'item_id_encoded1'
'item_id_encoded1' and 'item_id_encoded2' may hugely vary due to rare categories. No, it has nothing to do with categorical sizes.

================================================================


