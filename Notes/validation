1. Validation helps us evaluate a quality of the model
2. Validation helps us select the model which will perform
best on the unseen data
3. Underfitting refers to not capturing enough patterns in
the data
4. Generally, overfitting refers to
a. capturing noize
b. capturing patterns which do not generalize to test
data
5. In competitions, overfitting refers to
a. low model’s quality on test data,
which was unexpected due to validation scores
====================
There are three main validation strategies:
1. Holdout
2. KFold
3. Leave one out
Stratification preserve the same target distribution over different folds, validation more stable and especially useful for small and unbalanced data sets.
====================
The main rule you should know — never use data you train on to measure the quality of your model. The trick is to split all your data into training and validation parts.

Below you will find several ways to validate a model.

a) Holdout scheme:

Split train data into two parts: partA and partB.
Fit the model on partA, predict for partB.
Use predictions for partB for estimating model quality. Find such hyper-parameters, that quality on partB is maximized.

b) K-Fold scheme:

Split train data into K folds.
Iterate though each fold: retrain the model on all folds except current fold, predict for the current fold.
Use the predictions to calculate quality on each fold. Find such hyper-parameters, that quality on each fold is maximized. You can also estimate mean and variance of the loss. This is very helpful in order to understand significance of improvement.

c) LOO (Leave-One-Out) scheme:

Iterate over samples: retrain the model on all samples except current sample, predict for the current sample. You will need to retrain the model N times (if N is the number of samples in the dataset).
In the end you will get LOO predictions for every sample in the trainset and can calculate loss.
Notice, that these are validation schemes are supposed to be used to estimate quality of the model. When you found the right hyper-parameters and want to get test predictions don't forget to retrain your model using all training data.
==============================


DATA SPLIT
Different splitting strategies can differ significantly
1. in generated features
2. in a way the model will rely on that features
3. in some kind of target leak

1. In most cases data is split by
a. Row number
b. Time
c. Id
2. Logic of feature generation depends on the data
splitting strategy
3. Set up your validation to mimic the train/test split of
the competition

==============================
Causes of different scores and optimal parameters
1. Too little data
2. Too diverse and inconsistent data
We should do extensive validation
1. Average scores from different KFold splits
2. Tune model on one split, evaluate score on the other

Causes of validation problems:
• too little data in public leaderboard
• incorrect train/test split
• different distributions in train and test

• If we have big dispersion of scores on validation stage,
we should do extensive validation
– Average scores from different KFold splits
– Tune model on one split, evaluate score on the other
If submission’s score do not match local validation score,
we should
– Check if we have too little data in public LB
– Check if we overfitted
– Check if we chose correct splitting strategy
– Check if train/test have different distibutions
Expect LB shuffle because of
– Randomness
– Little amount of data
– Different public/private distributions
===============================
Validation in Sklearn
Advices on validation in a competition
===============================






